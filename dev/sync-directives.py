#!/usr/bin/env python3
"""
AIFP Directive Sync Manager ‚Äî Schema v2.0
------------------------------------------
Synchronizes all directive JSON definitions, helper functions, and directive flows
with the aifp_core.db database.

Location: dev/sync-directives.py
Run from project root or dev/ directory.

Handles:
- Directives (FP Core + FP Aux + Project + User Preferences + User System + Git)
- Categories (from directive JSON) ‚Üí categories table ‚Üí directive_categories junction
- Intent Keywords (from directive JSON) ‚Üí intent_keywords table ‚Üí directives_intent_keywords junction
- Helper Functions (from dev/helpers-json/*.json) ‚Üí helper_functions table
- Directive-Helper Mappings (from helper's used_by_directives field) ‚Üí directive_helpers junction
- Directive Flows (from directive_flow_*.json) ‚Üí directive_flow table
- Parent relationships
- Integrity Validation (post-sync verification)

Updated: 2026-02-04
- Relocated to dev/ directory (permanent dev staging area)
- Updated paths: dev/directives-json/, dev/helpers-json/, dev/logs/
- All paths now use __file__ based resolution for reliability

Total Directives: 125+ (30 FP Core + 36 FP Aux + 36 Project + 7 User Pref + 9 User System + 6 Git + ...)

This version aligns with the full schema (v2.0) for aifp_core.db.
"""

import os
import json
import sqlite3
from typing import List, Dict, Any, Set, Tuple
from pathlib import Path

# ===================================
# PATH RESOLUTION (based on script location)
# ===================================

# Script location: dev/sync-directives.py
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)  # One level up from dev/

# ===================================
# CONFIGURATION
# ===================================

# Database path - configurable via environment variable
# Default: src/aifp/database/aifp_core.db (production location)
# Override with AIFP_CORE_DB_PATH environment variable if needed
DB_PATH = os.environ.get(
    "AIFP_CORE_DB_PATH",
    os.path.join(PROJECT_ROOT, "src", "aifp", "database", "aifp_core.db")
)

# Directive JSON files are in dev/directives-json/
DIRECTIVES_JSON_DIR = os.path.join(SCRIPT_DIR, "directives-json")

FP_DIRECTIVE_FILES = [
    "directives-fp-core.json",
    "directives-fp-aux.json"
]

PROJECT_DIRECTIVE_FILES = [
    "directives-project.json"
]

USER_PREFERENCE_FILES = [
    "directives-user-pref.json"
]

USER_SYSTEM_FILES = [
    "directives-user-system.json"
]

GIT_DIRECTIVE_FILES = [
    "directives-git.json"
]

# Helpers are in dev/helpers-json/
HELPERS_DIR = os.path.join(SCRIPT_DIR, "helpers-json")

# Directive flows are in dev/directives-json/
DIRECTIVE_FLOW_FILES = [
    "directive_flow_fp.json",
    "directive_flow_project.json",
    "directive_flow_user_preferences.json"
]

# Migrations in dev/migrations/
MIGRATIONS_DIR = os.path.join(SCRIPT_DIR, "migrations")

# Sync report in dev/logs/
SYNC_REPORT_FILE = os.path.join(SCRIPT_DIR, "logs", "sync_report.json")

CURRENT_SCHEMA_VERSION = "2.0"
DRY_RUN = False


# ===================================
# DB CONNECTION & SCHEMA CHECK
# ===================================

def get_conn(db_path: str) -> sqlite3.Connection:
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def ensure_schema(conn: sqlite3.Connection):
    """Load and execute schema from external SQL file (source of truth)."""
    schema_path = os.path.join(PROJECT_ROOT, 'src', 'aifp', 'database', 'schemas', 'aifp_core.sql')

    if not os.path.exists(schema_path):
        print(f"‚ùå Schema file not found: {schema_path}")
        print(f"   Please ensure src/aifp/database/schemas/aifp_core.sql exists")
        raise FileNotFoundError(f"Schema file missing: {schema_path}")

    print(f"üìÑ Loading schema from: {schema_path}")

    with open(schema_path, 'r', encoding='utf-8') as f:
        schema_sql = f.read()

    conn.executescript(schema_sql)
    conn.commit()
    print("‚úÖ Schema loaded successfully")


# ===================================
# MIGRATION SYSTEM
# ===================================

def get_current_db_version(conn: sqlite3.Connection) -> str:
    """Get current schema version from database."""
    cur = conn.cursor()
    try:
        cur.execute("SELECT version FROM schema_version WHERE id = 1")
        row = cur.fetchone()
        return row['version'] if row else "1.0"
    except sqlite3.OperationalError:
        # schema_version table doesn't exist yet
        return "1.0"


def set_db_version(conn: sqlite3.Connection, version: str):
    """Update schema version in database."""
    cur = conn.cursor()
    cur.execute("""
        INSERT OR REPLACE INTO schema_version (id, version, updated_at)
        VALUES (1, ?, CURRENT_TIMESTAMP)
    """, (version,))
    conn.commit()


def get_migration_files() -> List[tuple]:
    """
    Get all migration files from migrations directory.
    Returns list of (version, filepath) tuples sorted by version.

    Migration files should be named: migration_1.2_to_1.3.sql
    """
    if not os.path.exists(MIGRATIONS_DIR):
        return []

    migrations = []
    for filename in os.listdir(MIGRATIONS_DIR):
        if filename.endswith('.sql') and filename.startswith('migration_'):
            # Parse version from filename: migration_1.2_to_1.3.sql -> 1.3
            try:
                parts = filename.replace('.sql', '').split('_to_')
                if len(parts) == 2:
                    target_version = parts[1]
                    migrations.append((target_version, os.path.join(MIGRATIONS_DIR, filename)))
            except Exception:
                print(f"‚ö†Ô∏è  Could not parse migration filename: {filename}")

    # Sort by version
    migrations.sort(key=lambda x: [int(n) for n in x[0].split('.')])
    return migrations


def run_migrations(conn: sqlite3.Connection):
    """
    Run all pending migrations to bring database to current schema version.
    Ensures backward compatibility when schema changes.
    """
    current_version = get_current_db_version(conn)
    print(f"üìä Current database schema version: {current_version}")

    if current_version == CURRENT_SCHEMA_VERSION:
        print(f"‚úÖ Database already at current version ({CURRENT_SCHEMA_VERSION})")
        return

    print(f"üîÑ Migrating database from {current_version} to {CURRENT_SCHEMA_VERSION}")

    migrations = get_migration_files()
    if not migrations:
        print("‚ö†Ô∏è  No migration files found. Using ensure_schema() for initialization.")
        return

    # Run migrations that are newer than current version
    for target_version, migration_file in migrations:
        # Compare versions
        current_parts = [int(n) for n in current_version.split('.')]
        target_parts = [int(n) for n in target_version.split('.')]

        # Only run if target > current
        if target_parts > current_parts:
            print(f"  ‚Üí Applying migration to {target_version}: {os.path.basename(migration_file)}")

            try:
                with open(migration_file, 'r', encoding='utf-8') as f:
                    migration_sql = f.read()

                conn.executescript(migration_sql)
                set_db_version(conn, target_version)
                print(f"  ‚úÖ Migration to {target_version} complete")

            except Exception as e:
                print(f"  ‚ùå Migration to {target_version} failed: {e}")
                if not DRY_RUN:
                    conn.rollback()
                raise

    # Update to current version
    if get_current_db_version(conn) != CURRENT_SCHEMA_VERSION:
        set_db_version(conn, CURRENT_SCHEMA_VERSION)

    print(f"‚úÖ Database migrated to version {CURRENT_SCHEMA_VERSION}")


# ===================================
# CORE HELPERS
# ===================================

def load_json_file(filepath: str) -> List[Dict[str, Any]]:
    if not os.path.exists(filepath):
        print(f"‚ö†Ô∏è  Missing file: {filepath}")
        return []
    with open(filepath, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
            # Handle different JSON structures:
            # - Direct array of directives: [...]
            # - Wrapper object with 'directives' key: {"directives": [...], ...}
            # - Wrapper object with 'helpers' key: {"helpers": [...], ...}
            # - Wrapper object with 'flows' key: {"flows": [...], ...}
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                if 'directives' in data:
                    return data['directives']
                elif 'helpers' in data:
                    return data['helpers']
                elif 'flows' in data:
                    return data['flows']
                else:
                    return [data]
            else:
                return []
        except Exception as e:
            print(f"‚ùå Error parsing {filepath}: {e}")
            return []


# ===================================
# CATEGORY MANAGEMENT
# ===================================

def extract_categories_from_directives(all_entries: List[Dict[str, Any]]) -> Set[Tuple[str, str]]:
    """
    Extract unique categories from directive entries.
    Returns set of (name, description) tuples.
    """
    categories = set()
    for entry in all_entries:
        if "category" in entry and entry["category"]:
            cat = entry["category"]
            if isinstance(cat, dict):
                name = cat.get("name", "")
                description = cat.get("description", "")
                if name:
                    categories.add((name, description))
    return categories


def sync_categories(conn: sqlite3.Connection, categories: Set[Tuple[str, str]]) -> Dict[str, int]:
    """
    Insert categories into categories table.
    Returns mapping of category_name -> category_id.
    """
    print(f"\nüìÇ Syncing {len(categories)} categories...")

    cur = conn.cursor()
    category_id_map = {}
    inserted = 0

    # Sort categories alphabetically by name for consistent ordering
    for name, description in sorted(categories, key=lambda x: x[0]):
        # Insert or ignore
        cur.execute("""
            INSERT OR IGNORE INTO categories (name, description)
            VALUES (?, ?)
        """, (name, description))

        # Get the id
        cur.execute("SELECT id FROM categories WHERE name = ?", (name,))
        row = cur.fetchone()
        if row:
            category_id_map[name] = row['id']
            if cur.rowcount > 0:
                inserted += 1

    conn.commit()
    print(f"‚úÖ Categories synced: {inserted} new, {len(categories) - inserted} existing")
    return category_id_map


def link_directive_categories(conn: sqlite3.Connection, all_entries: List[Dict[str, Any]],
                               category_id_map: Dict[str, int]):
    """
    Link directives to categories via directive_categories junction table.
    """
    print(f"\nüîó Linking directives to categories...")

    cur = conn.cursor()
    linked = 0

    for entry in all_entries:
        if "category" not in entry or not entry["category"]:
            continue

        directive_name = entry.get("name")
        if not directive_name:
            continue

        # Get directive_id
        cur.execute("SELECT id FROM directives WHERE name = ?", (directive_name,))
        directive_row = cur.fetchone()
        if not directive_row:
            continue

        directive_id = directive_row['id']

        # Get category_id
        cat = entry["category"]
        if isinstance(cat, dict):
            cat_name = cat.get("name", "")
            if cat_name in category_id_map:
                category_id = category_id_map[cat_name]

                # Insert link (ignore if exists)
                cur.execute("""
                    INSERT OR IGNORE INTO directive_categories (directive_id, category_id)
                    VALUES (?, ?)
                """, (directive_id, category_id))

                if cur.rowcount > 0:
                    linked += 1

    conn.commit()
    print(f"‚úÖ Linked {linked} directive-category relationships")


# ===================================
# INTENT KEYWORD MANAGEMENT
# ===================================

def extract_intent_keywords_from_directives(all_entries: List[Dict[str, Any]]) -> Set[str]:
    """
    Extract unique intent keywords from directive entries.
    Returns set of keywords.
    """
    keywords = set()
    for entry in all_entries:
        if "intent_keywords_json" in entry and entry["intent_keywords_json"]:
            kw_list = entry["intent_keywords_json"]
            if isinstance(kw_list, list):
                for kw in kw_list:
                    if isinstance(kw, str) and kw.strip():
                        keywords.add(kw.strip().lower())
    return keywords


def sync_intent_keywords(conn: sqlite3.Connection, keywords: Set[str]) -> Dict[str, int]:
    """
    Insert intent keywords into intent_keywords table.
    Returns mapping of keyword -> keyword_id.
    """
    print(f"\nüîë Syncing {len(keywords)} intent keywords...")

    cur = conn.cursor()
    keyword_id_map = {}
    inserted = 0

    # Sort keywords alphabetically for consistent ordering
    for keyword in sorted(keywords):
        # Insert or ignore
        cur.execute("""
            INSERT OR IGNORE INTO intent_keywords (keyword)
            VALUES (?)
        """, (keyword,))

        # Get the id
        cur.execute("SELECT id FROM intent_keywords WHERE keyword = ?", (keyword,))
        row = cur.fetchone()
        if row:
            keyword_id_map[keyword] = row['id']
            if cur.rowcount > 0:
                inserted += 1

    conn.commit()
    print(f"‚úÖ Intent keywords synced: {inserted} new, {len(keywords) - inserted} existing")
    return keyword_id_map


def link_directive_intent_keywords(conn: sqlite3.Connection, all_entries: List[Dict[str, Any]],
                                   keyword_id_map: Dict[str, int]):
    """
    Link directives to intent keywords via directives_intent_keywords junction table.
    """
    print(f"\nüîó Linking directives to intent keywords...")

    cur = conn.cursor()
    linked = 0

    for entry in all_entries:
        if "intent_keywords_json" not in entry or not entry["intent_keywords_json"]:
            continue

        directive_name = entry.get("name")
        if not directive_name:
            continue

        # Get directive_id
        cur.execute("SELECT id FROM directives WHERE name = ?", (directive_name,))
        directive_row = cur.fetchone()
        if not directive_row:
            continue

        directive_id = directive_row['id']

        # Link keywords
        kw_list = entry["intent_keywords_json"]
        if isinstance(kw_list, list):
            for kw in kw_list:
                if isinstance(kw, str) and kw.strip():
                    kw_normalized = kw.strip().lower()
                    if kw_normalized in keyword_id_map:
                        keyword_id = keyword_id_map[kw_normalized]

                        # Insert link (ignore if exists)
                        cur.execute("""
                            INSERT OR IGNORE INTO directives_intent_keywords (directive_id, keyword_id)
                            VALUES (?, ?)
                        """, (directive_id, keyword_id))

                        if cur.rowcount > 0:
                            linked += 1

    conn.commit()
    print(f"‚úÖ Linked {linked} directive-keyword relationships")


# ===================================
# INSERT / UPDATE FUNCTIONS
# ===================================

def upsert_directive(conn, entry: Dict[str, Any]) -> str:
    cur = conn.cursor()
    cur.execute("SELECT id FROM directives WHERE name=?", (entry["name"],))
    existing = cur.fetchone()

    # Note: intent_keywords_json and category are NOT in directives table anymore
    # They are in separate tables with junction tables
    fields = (
        entry["name"],
        entry["type"],
        entry.get("level"),
        entry.get("parent_directive"),
        entry.get("description"),
        json.dumps(entry.get("workflow", {})),
        entry.get("md_file_path"),
        json.dumps(entry.get("roadblocks_json", [])),
        entry.get("confidence_threshold", 0.5)
    )

    if existing:
        cur.execute("""
            UPDATE directives SET
                type=?, level=?, parent_directive=?, description=?, workflow=?,
                md_file_path=?, roadblocks_json=?,
                confidence_threshold=?
            WHERE name=?
        """, fields[1:] + (entry["name"],))
        conn.commit()
        return "updated"
    else:
        cur.execute("""
            INSERT INTO directives
            (name, type, level, parent_directive, description, workflow,
             md_file_path, roadblocks_json, confidence_threshold)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, fields)
        conn.commit()
        return "added"


# ===================================
# HELPER FUNCTIONS SYNC
# ===================================

def load_all_helper_files() -> List[Dict[str, Any]]:
    """
    Load all helper JSON files from dev/helpers-json/ directory.
    Returns combined list of all helpers.
    """
    if not os.path.exists(HELPERS_DIR):
        print(f"‚ö†Ô∏è  Helpers directory not found: {HELPERS_DIR}")
        return []

    all_helpers = []
    helper_files = sorted(Path(HELPERS_DIR).glob("helpers-*.json"))

    print(f"\nüìö Loading helper files from {HELPERS_DIR}")
    for helper_file in helper_files:
        helpers = load_json_file(str(helper_file))
        if helpers:
            all_helpers.extend(helpers)
            print(f"   üìò Loaded {len(helpers)} helpers from {helper_file.name}")

    return all_helpers


def sync_helper_functions(conn: sqlite3.Connection) -> int:
    """
    Load helper functions from all JSON files in dev/helpers-json/ and populate helper_functions table.

    Populates table with complete helper data:
    - name, file_path, parameters, purpose, error_handling
    - is_tool: Read from JSON (TRUE if exposed as MCP tool)
    - is_sub_helper: Read from JSON (TRUE if internal helper only)
    - target_database: Read from JSON
    - return_statements: Read from JSON

    Source: dev/helpers-json/helpers-*.json
    """
    print("\nüîß Syncing helper functions from JSON files...")

    helpers = load_all_helper_files()

    if not helpers:
        print("‚ö†Ô∏è  No helpers found in JSON files")
        return 0

    print(f"üìã Processing {len(helpers)} helper functions")

    # Insert helper functions into table
    cur = conn.cursor()
    inserted = 0
    updated = 0

    for helper in helpers:
        name = helper.get('name')
        if not name:
            continue

        # Check if helper already exists
        cur.execute("SELECT id FROM helper_functions WHERE name=?", (name,))
        existing = cur.fetchone()

        # Convert parameters to JSON string if it's a list
        parameters = helper.get('parameters')
        if isinstance(parameters, list):
            parameters = json.dumps(parameters)
        elif isinstance(parameters, str):
            parameters = parameters  # Already JSON string
        else:
            parameters = json.dumps([])

        # Convert return_statements to JSON string if it's a list
        return_statements = helper.get('return_statements')
        if isinstance(return_statements, list):
            return_statements = json.dumps(return_statements)
        elif isinstance(return_statements, str):
            return_statements = return_statements
        else:
            return_statements = json.dumps([])

        if existing:
            # Update existing helper with latest data from JSON
            cur.execute("""
                UPDATE helper_functions
                SET file_path = ?,
                    parameters = ?,
                    purpose = ?,
                    error_handling = ?,
                    is_tool = ?,
                    is_sub_helper = ?,
                    return_statements = ?,
                    target_database = ?
                WHERE name = ?
            """, (
                helper.get('file_path'),
                parameters,
                helper.get('purpose'),
                helper.get('error_handling'),
                1 if helper.get('is_tool', False) else 0,
                1 if helper.get('is_sub_helper', False) else 0,
                return_statements,
                helper.get('target_database'),
                name
            ))
            updated += 1
        else:
            # Insert new helper with all available data
            cur.execute("""
                INSERT INTO helper_functions
                (name, file_path, parameters, purpose, error_handling, is_tool, is_sub_helper,
                 return_statements, target_database)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                name,
                helper.get('file_path'),
                parameters,
                helper.get('purpose'),
                helper.get('error_handling'),
                1 if helper.get('is_tool', False) else 0,
                1 if helper.get('is_sub_helper', False) else 0,
                return_statements,
                helper.get('target_database')
            ))
            inserted += 1

    conn.commit()
    print(f"‚úÖ Helper functions synced: {inserted} new, {updated} updated")

    # Show module organization summary
    cur.execute("""
        SELECT target_database, COUNT(*) as count
        FROM helper_functions
        GROUP BY target_database
        ORDER BY target_database
    """)
    print("   Database organization:")
    for row in cur.fetchall():
        print(f"     {row['count']:2d} helpers ‚Üí {row['target_database']}")

    return inserted + updated


def sync_directive_helper_mappings(conn: sqlite3.Connection, all_helpers: List[Dict[str, Any]]):
    """
    Populate directive_helpers junction table using used_by_directives field from helper JSONs.

    Each helper's used_by_directives field contains:
    [
      {
        "directive_name": "aifp_run",
        "execution_context": "self_invocation",
        "sequence_order": 1,
        "is_required": true,
        "parameters_mapping": {},
        "description": "..."
      }
    ]
    """
    print("\nüîó Syncing directive-helper mappings from helper JSONs...")

    cur = conn.cursor()
    inserted = 0
    skipped = 0
    errors = 0

    for helper in all_helpers:
        helper_name = helper.get('name')
        if not helper_name:
            continue

        used_by = helper.get('used_by_directives', [])
        if not used_by or not isinstance(used_by, list):
            continue

        # Look up helper_function_id
        cur.execute("SELECT id FROM helper_functions WHERE name=?", (helper_name,))
        helper_row = cur.fetchone()
        if not helper_row:
            print(f"   ‚ö†Ô∏è  Helper not found in database: {helper_name}")
            errors += 1
            continue

        helper_id = helper_row['id']

        # Process each directive mapping
        for mapping in used_by:
            directive_name = mapping.get('directive_name')
            if not directive_name:
                skipped += 1
                continue

            # Look up directive_id
            cur.execute("SELECT id FROM directives WHERE name=?", (directive_name,))
            directive_row = cur.fetchone()
            if not directive_row:
                print(f"   ‚ö†Ô∏è  Directive not found: {directive_name}")
                errors += 1
                continue

            directive_id = directive_row['id']

            # Extract mapping fields
            execution_context = mapping.get('execution_context', '')
            sequence_order = mapping.get('sequence_order', 0)
            is_required = 1 if mapping.get('is_required', True) else 0
            parameters_mapping = mapping.get('parameters_mapping', {})
            description = mapping.get('description', '')

            # Convert parameters_mapping to JSON string if needed
            if isinstance(parameters_mapping, dict):
                parameters_mapping = json.dumps(parameters_mapping)

            # Check if mapping already exists
            cur.execute("""
                SELECT id FROM directive_helpers
                WHERE directive_id=? AND helper_function_id=? AND execution_context=?
            """, (directive_id, helper_id, execution_context))

            existing = cur.fetchone()

            if existing:
                # Update existing mapping
                cur.execute("""
                    UPDATE directive_helpers
                    SET sequence_order = ?,
                        is_required = ?,
                        parameters_mapping = ?,
                        description = ?
                    WHERE id = ?
                """, (
                    sequence_order,
                    is_required,
                    parameters_mapping,
                    description,
                    existing['id']
                ))
            else:
                # Insert new mapping
                cur.execute("""
                    INSERT INTO directive_helpers
                    (directive_id, helper_function_id, execution_context, sequence_order,
                     is_required, parameters_mapping, description)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    directive_id,
                    helper_id,
                    execution_context,
                    sequence_order,
                    is_required,
                    parameters_mapping,
                    description
                ))
                inserted += 1

    conn.commit()
    print(f"‚úÖ Directive-helper mappings synced: {inserted} new")
    if errors > 0:
        print(f"   ‚ö†Ô∏è  {errors} mappings had errors (directive or helper not found)")
    if skipped > 0:
        print(f"   ‚ö†Ô∏è  {skipped} mappings skipped (missing required fields)")

    # Show summary statistics
    cur.execute("""
        SELECT COUNT(*) as total_mappings,
               COUNT(DISTINCT directive_id) as unique_directives,
               COUNT(DISTINCT helper_function_id) as unique_helpers
        FROM directive_helpers
    """)
    stats = cur.fetchone()
    print(f"   Total mappings in database: {stats['total_mappings']}")
    print(f"   Directives with helpers: {stats['unique_directives']}")
    print(f"   Helpers mapped to directives: {stats['unique_helpers']}")


# ===================================
# DIRECTIVE FLOW SYNC
# ===================================

def sync_directive_flows(conn: sqlite3.Connection):
    """
    Load directive flows from directive_flow_*.json files and populate directive_flow table.

    Flow structure:
    {
      "from_directive": "aifp_run",
      "to_directive": "aifp_status",
      "flow_type": "status_branch",
      "flow_category": "project",
      "condition_key": "is_new_session",
      "condition_value": "true",
      "condition_description": "...",
      "priority": 100,
      "description": "..."
    }
    """
    print("\nüîÑ Syncing directive flows from JSON files...")

    # Always fresh - delete all existing flows before inserting
    cur = conn.cursor()
    cur.execute("DELETE FROM directive_flow")
    deleted_count = cur.rowcount
    if deleted_count > 0:
        print(f"   üóëÔ∏è  Cleared {deleted_count} existing flows (fresh sync)")

    all_flows = []
    for flow_file in DIRECTIVE_FLOW_FILES:
        flow_path = os.path.join(DIRECTIVES_JSON_DIR, flow_file)
        flows = load_json_file(flow_path)
        if flows:
            all_flows.extend(flows)
            print(f"   üìò Loaded {len(flows)} flows from {flow_file}")

    if not all_flows:
        print("‚ö†Ô∏è  No directive flows found")
        return

    print(f"üìã Processing {len(all_flows)} directive flows")

    inserted = 0
    errors = 0

    for flow in all_flows:
        from_directive = flow.get('from_directive')
        to_directive = flow.get('to_directive')
        flow_type = flow.get('flow_type', 'conditional')

        if not to_directive:
            errors += 1
            continue

        # Allow wildcard '*' for reference_consultation and utility flows
        if from_directive == '*':
            if flow_type not in ['reference_consultation', 'utility', 'conditional']:
                print(f"   ‚ö†Ô∏è  Wildcard '*' only allowed for reference_consultation, utility, or conditional flows: {to_directive}")
                errors += 1
                continue
            # Wildcard is valid - only verify to_directive exists
            cur.execute("SELECT name FROM directives WHERE name = ?", (to_directive,))
            if not cur.fetchone():
                print(f"   ‚ö†Ô∏è  Target directive not found: {to_directive}")
                errors += 1
                continue
        else:
            # Normal flow - verify both directives exist
            if not from_directive:
                errors += 1
                continue

            cur.execute("SELECT name FROM directives WHERE name IN (?, ?)",
                       (from_directive, to_directive))
            found = [row['name'] for row in cur.fetchall()]

            if from_directive not in found:
                print(f"   ‚ö†Ô∏è  Source directive not found: {from_directive}")
                errors += 1
                continue

            if to_directive not in found:
                print(f"   ‚ö†Ô∏è  Target directive not found: {to_directive}")
                errors += 1
                continue

        # Extract flow fields
        flow_category = flow.get('flow_category', 'project')
        condition_key = flow.get('condition_key')
        condition_value = flow.get('condition_value')
        condition_description = flow.get('condition_description')
        priority = flow.get('priority', 0)
        description = flow.get('description', '')

        # Insert flow (table was cleared at start for fresh sync)
        cur.execute("""
            INSERT INTO directive_flow
            (from_directive, to_directive, flow_category, flow_type,
             condition_key, condition_value, condition_description, priority, description)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            from_directive,
            to_directive,
            flow_category,
            flow_type,
            condition_key,
            condition_value,
            condition_description,
            priority,
            description
        ))
        inserted += 1

    conn.commit()
    print(f"‚úÖ Directive flows synced: {inserted} inserted (fresh sync)")
    if errors > 0:
        print(f"   ‚ö†Ô∏è  {errors} flows had errors (missing directives or fields)")

    # Show summary statistics
    cur.execute("""
        SELECT flow_category, flow_type, COUNT(*) as count
        FROM directive_flow
        GROUP BY flow_category, flow_type
        ORDER BY flow_category, flow_type
    """)
    print("   Flow organization:")
    for row in cur.fetchall():
        print(f"     {row['count']:2d} flows ‚Üí {row['flow_category']}/{row['flow_type']}")


# ===================================
# SYNC EXECUTION
# ===================================

def sync_directives():
    print("üîÑ Starting Full AIFP Directive Sync (Schema v2.0)")
    print("üì¶ Including: FP Core, FP Aux, Project, User Prefs, User System, Git Integration")
    print("üì¶ Including: Categories, Intent Keywords, Helpers, Directive-Helper Mappings, Directive Flows")

    # Ensure database directory exists
    db_dir = os.path.dirname(DB_PATH)
    if db_dir and not os.path.exists(db_dir):
        os.makedirs(db_dir, exist_ok=True)
        print(f"üìÅ Created database directory: {db_dir}")

    conn = get_conn(DB_PATH)
    ensure_schema(conn)

    # Run any pending migrations
    run_migrations(conn)

    cur = conn.cursor()

    # Load all directive files
    all_directive_files = (
        FP_DIRECTIVE_FILES +
        PROJECT_DIRECTIVE_FILES +
        USER_PREFERENCE_FILES +
        USER_SYSTEM_FILES +
        GIT_DIRECTIVE_FILES
    )

    all_entries = []
    for file in all_directive_files:
        file_path = os.path.join(DIRECTIVES_JSON_DIR, file)
        entries = load_json_file(file_path)
        all_entries.extend(entries)
        print(f"üìò Loaded {len(entries)} directives from {file}")

    # Extract and sync categories FIRST
    categories = extract_categories_from_directives(all_entries)
    category_id_map = sync_categories(conn, categories)

    # Extract and sync intent keywords FIRST
    intent_keywords = extract_intent_keywords_from_directives(all_entries)
    keyword_id_map = sync_intent_keywords(conn, intent_keywords)

    # Sync directives
    report = {"added": [], "updated": []}

    for entry in all_entries:
        if not entry.get("name") or not entry.get("type"):
            continue
        result = upsert_directive(conn, entry)
        report[result].append(entry["name"])

    # Second pass: Parent linkage
    for entry in all_entries:
        if entry.get("parent_directive"):
            cur.execute("""
                UPDATE directives SET parent_directive=?
                WHERE name=? AND parent_directive IS NULL
            """, (entry["parent_directive"], entry["name"]))
    conn.commit()

    # Link directives to categories
    link_directive_categories(conn, all_entries, category_id_map)

    # Link directives to intent keywords
    link_directive_intent_keywords(conn, all_entries, keyword_id_map)

    # Sync helper functions from JSON files
    all_helpers = load_all_helper_files()
    sync_helper_functions(conn)

    # Sync directive-helper mappings from helper's used_by_directives field
    sync_directive_helper_mappings(conn, all_helpers)

    # Sync directive flows
    sync_directive_flows(conn)

    if DRY_RUN:
        conn.rollback()
        print("üß™ Dry-run mode: no DB changes committed.")
    else:
        conn.commit()

    print(f"\n‚úÖ Directives: {len(report['added'])} added | {len(report['updated'])} updated")
    print(f"‚úÖ Categories: {len(categories)}")
    print(f"‚úÖ Intent Keywords: {len(intent_keywords)}")
    print(f"‚úÖ Helpers: {len(all_helpers)}")

    os.makedirs(os.path.dirname(SYNC_REPORT_FILE), exist_ok=True)
    with open(SYNC_REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2)
    print(f"üìÑ Sync report saved to {SYNC_REPORT_FILE}")

    # Run post-sync validation
    validate_integrity(conn)

    conn.close()


# ===================================
# INTEGRITY VALIDATION LAYER
# ===================================

def validate_integrity(conn):
    """Runs a suite of consistency checks after sync."""
    print("\nüß© Running Post-Sync Integrity Validation...")
    cur = conn.cursor()
    issues = []

    # 1. Check for FP directives with levels
    cur.execute("SELECT name FROM directives WHERE type='fp' AND level IS NOT NULL;")
    for row in cur.fetchall():
        issues.append(f"‚ùå FP directive '{row['name']}' has a level assigned.")

    # 2. Check for orphaned parent_directive links
    cur.execute("""
        SELECT name, parent_directive FROM directives
        WHERE parent_directive IS NOT NULL
        AND parent_directive NOT IN (SELECT name FROM directives)
    """)
    for row in cur.fetchall():
        issues.append(f"‚ö†Ô∏è Orphaned parent link: {row['name']} ‚Üí {row['parent_directive']}")

    # 3. Check for duplicate category links
    cur.execute("""
        SELECT directive_id, category_id, COUNT(*) as c FROM directive_categories
        GROUP BY directive_id, category_id HAVING c > 1
    """)
    for row in cur.fetchall():
        issues.append(f"‚ö†Ô∏è Duplicate directive-category link for directive_id={row['directive_id']}.")

    # 4. Verify helper_functions loaded from JSON with is_tool and is_sub_helper
    cur.execute("SELECT COUNT(*) as tool_count FROM helper_functions WHERE is_tool = 1;")
    tool_count = cur.fetchone()["tool_count"]
    cur.execute("SELECT COUNT(*) as sub_helper_count FROM helper_functions WHERE is_sub_helper = 1;")
    sub_helper_count = cur.fetchone()["sub_helper_count"]

    if tool_count == 0:
        issues.append("‚ö†Ô∏è No helpers marked as is_tool=1. MCP tools may not be properly configured.")
    else:
        print(f"   ‚úì Found {tool_count} MCP tools (is_tool=1)")

    if sub_helper_count > 0:
        print(f"   ‚úì Found {sub_helper_count} sub-helpers (is_sub_helper=1)")

    # 5. Verify directive_helpers junction table has entries
    cur.execute("SELECT COUNT(*) as mapping_count FROM directive_helpers;")
    mapping_count = cur.fetchone()["mapping_count"]
    if mapping_count == 0:
        issues.append("‚ö†Ô∏è No directive-helper mappings found. Check helper JSONs for used_by_directives field.")
    else:
        print(f"   ‚úì Found {mapping_count} directive-helper mappings")

    # 6. Verify directive_flow table has entries
    cur.execute("SELECT COUNT(*) as flow_count FROM directive_flow;")
    flow_count = cur.fetchone()["flow_count"]
    if flow_count == 0:
        issues.append("‚ö†Ô∏è No directive flows found. Check directive_flow_*.json files.")
    else:
        print(f"   ‚úì Found {flow_count} directive flows")

    # 7. Verify all directives have valid workflow structure
    cur.execute("SELECT name, workflow FROM directives;")
    for row in cur.fetchall():
        try:
            workflow = json.loads(row['workflow']) if isinstance(row['workflow'], str) else row['workflow']
            if not isinstance(workflow, dict) or 'trunk' not in workflow:
                issues.append(f"‚ö†Ô∏è Directive '{row['name']}' missing required 'trunk' in workflow")
        except (json.JSONDecodeError, TypeError):
            issues.append(f"‚ùå Directive '{row['name']}' has malformed workflow JSON")

    # 8. Verify MD file paths exist for all directives
    cur.execute("SELECT name, md_file_path FROM directives WHERE md_file_path IS NOT NULL;")
    md_files_checked = 0

    # Use PROJECT_ROOT for reference directory
    reference_dir = os.path.join(PROJECT_ROOT, 'src', 'aifp', 'reference')

    for row in cur.fetchall():
        # Construct absolute path
        md_path = os.path.join(reference_dir, row['md_file_path'])
        if not os.path.exists(md_path):
            issues.append(f"‚ùå Directive '{row['name']}' references missing MD file: {row['md_file_path']}")
        md_files_checked += 1

    if md_files_checked > 0:
        print(f"   ‚úì Verified {md_files_checked} MD file paths")

    # 9. Verify categories table populated
    cur.execute("SELECT COUNT(*) as cat_count FROM categories;")
    cat_count = cur.fetchone()["cat_count"]
    if cat_count == 0:
        issues.append("‚ö†Ô∏è No categories found. Check directive JSON files for category field.")
    else:
        print(f"   ‚úì Found {cat_count} categories")

    # 10. Verify intent_keywords table populated
    cur.execute("SELECT COUNT(*) as kw_count FROM intent_keywords;")
    kw_count = cur.fetchone()["kw_count"]
    if kw_count == 0:
        issues.append("‚ö†Ô∏è No intent keywords found. Check directive JSON files for intent_keywords_json field.")
    else:
        print(f"   ‚úì Found {kw_count} intent keywords")

    # 11. Verify directive_categories links
    cur.execute("SELECT COUNT(*) as link_count FROM directive_categories;")
    dc_link_count = cur.fetchone()["link_count"]
    if dc_link_count == 0:
        issues.append("‚ö†Ô∏è No directive-category links found.")
    else:
        print(f"   ‚úì Found {dc_link_count} directive-category links")

    # 12. Verify directives_intent_keywords links
    cur.execute("SELECT COUNT(*) as link_count FROM directives_intent_keywords;")
    dik_link_count = cur.fetchone()["link_count"]
    if dik_link_count == 0:
        issues.append("‚ö†Ô∏è No directive-keyword links found.")
    else:
        print(f"   ‚úì Found {dik_link_count} directive-keyword links")

    if issues:
        print(f"\n‚ùó Found {len(issues)} integrity warnings:")
        for i in issues:
            print("   " + i)
    else:
        print("\n‚úÖ Database passed all integrity checks cleanly.")

    print("üîç Integrity validation complete.\n")


# ===================================
# ENTRY POINT
# ===================================

if __name__ == "__main__":
    try:
        sync_directives()
    except Exception as e:
        print(f"‚ùå Sync failed: {e}")
        import traceback
        traceback.print_exc()

# ==================================
# MIGRATION GUIDE
# ==================================
# To create a new migration:
# 1. Create file: dev/migrations/migration_1.5_to_2.0.sql
# 2. Include schema changes (ALTER TABLE, CREATE TABLE, etc.)
# 3. Update CURRENT_SCHEMA_VERSION at top of this file
# 4. Run sync script - migrations apply automatically
#
# Example migration file structure:
# -- Migration from v1.5 to v2.0
# -- Description: Add new feature X
#
# ALTER TABLE directives ADD COLUMN new_field TEXT;
# CREATE INDEX IF NOT EXISTS idx_new_field ON directives(new_field);
#
# -- Update version (handled automatically by run_migrations)
#
# File Locations (permanent dev staging area):
# - Script:     dev/sync-directives.py
# - Directives: dev/directives-json/*.json
# - Helpers:    dev/helpers-json/*.json
# - Flows:      dev/directives-json/directive_flow_*.json
# - Migrations: dev/migrations/*.sql
# - Logs:       dev/logs/sync_report.json
# - Target DB:  src/aifp/database/aifp_core.db
# ==================================
